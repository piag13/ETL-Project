import io
import logging
import math
import random
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple

import boto3
import numpy as np
import pandas as pd
from botocore.client import Config
from sqlalchemy import text

from .config import settings
from .db import engine

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def get_s3_client():
    return boto3.client(
        "s3",
        endpoint_url=settings.s3_endpoint_url,
        aws_access_key_id=settings.aws_access_key_id,
        aws_secret_access_key=settings.aws_secret_access_key,
        region_name=settings.aws_region,
        config=Config(s3={"addressing_style": "path"}),
    )


def ensure_bucket():
    s3 = get_s3_client()
    buckets = s3.list_buckets().get("Buckets", [])
    if not any(b["Name"] == settings.s3_bucket_name for b in buckets):
        s3.create_bucket(Bucket=settings.s3_bucket_name)


def is_double_day(date: datetime) -> bool:
    """Ki·ªÉm tra xem c√≥ ph·∫£i ng√†y ƒë√¥i kh√¥ng (1/1, 2/2, ..., 12/12)"""
    return date.month == date.day and date.month <= 12


def is_sale_event(date: datetime) -> bool:
    """Ki·ªÉm tra xem c√≥ ph·∫£i ng√†y sale kh√¥ng"""
    # Black Friday (th·ª© 6 sau L·ªÖ T·∫° ∆°n - th∆∞·ªùng l√† ng√†y 23-29 th√°ng 11)
    if date.month == 11 and 23 <= date.day <= 29 and date.weekday() == 4:
        return True
    # Cyber Monday (th·ª© 2 sau Black Friday)
    if date.month == 11 and 26 <= date.day <= 30 and date.weekday() == 0:
        return True
    # End of month sale (3 ng√†y cu·ªëi th√°ng)
    if date.day >= 28:
        return True
    # Mid-month sale (ng√†y 15-17)
    if 15 <= date.day <= 17:
        return True
    return False


def generate_chunk(start_id: int, size: int) -> pd.DataFrame:
    """
    Generate a chunk of synthetic e-commerce sales data v·ªõi patterns t·ª± nhi√™n.
    
    Features:
    - E-commerce product categories
    - Ng√†y ƒë√¥i (1/1, 2/2, ..., 12/12) c√≥ l∆∞·ª£ng mua nhi·ªÅu
    - Sale events (Black Friday, Cyber Monday, end of month, etc.)
    - Country distribution v√† category preferences
    - User behavior patterns
    - Amount distribution theo product category
    """
    logger.info(f"Generating chunk: start_id={start_id}, size={size}")
    
    ids = np.arange(start_id, start_id + size, dtype="int64")
    
    # 1. Country distribution kh√¥ng ƒë·ªÅu (US v√† VN ph·ªï bi·∫øn h∆°n)
    country_weights = {
        "US": 0.25,  # 25%
        "VN": 0.20,  # 20%
        "JP": 0.12,  # 12%
        "DE": 0.10,  # 10%
        "FR": 0.10,  # 10%
        "GB": 0.10,  # 10%
        "SG": 0.08,  # 8%
        "AU": 0.05,  # 5%
    }
    countries_list = list(country_weights.keys())
    countries_probs = list(country_weights.values())
    countries = np.random.choice(countries_list, size=size, p=countries_probs)
    
    # 2. User IDs v·ªõi power users (m·ªôt s·ªë user xu·∫•t hi·ªán nhi·ªÅu h∆°n)
    # 80% users l√† regular, 20% l√† power users (xu·∫•t hi·ªán 3-5 l·∫ßn nhi·ªÅu h∆°n)
    regular_users = np.random.randint(1, 800000, size=int(size * 0.8))
    power_users = np.random.choice(
        np.arange(800000, 1000000), 
        size=int(size * 0.2),
        replace=True
    )
    user_ids = np.concatenate([regular_users, power_users])
    np.random.shuffle(user_ids)
    user_ids = user_ids[:size].astype("int32")
    
    # 3. E-commerce Product Categories - Distribution ph√π h·ª£p v·ªõi b√°n h√†ng online
    # Electronics v√† Fashion chi·∫øm ph·∫ßn l·ªõn, Home & Beauty trung b√¨nh, Books & Sports √≠t h∆°n
    global_category_distribution = {
        "electronics": 0.30,    # 30% - Nhi·ªÅu nh·∫•t, gi√° tr·ªã cao-trung b√¨nh
        "fashion": 0.28,        # 28% - Nhi·ªÅu th·ª© 2, gi√° tr·ªã trung b√¨nh
        "home": 0.18,          # 18% - Trung b√¨nh, gi√° tr·ªã trung b√¨nh-cao
        "beauty": 0.12,        # 12% - Trung b√¨nh-th·∫•p, gi√° tr·ªã th·∫•p-trung b√¨nh
        "books": 0.07,         # 7% - √çt, gi√° tr·ªã th·∫•p
        "sports": 0.05,        # 5% - √çt nh·∫•t, gi√° tr·ªã trung b√¨nh-cao
    }
    
    # Country-specific adjustments cho e-commerce
    country_category_adjustments = {
        "US": {"electronics": +0.03, "fashion": +0.02, "home": -0.01, "beauty": -0.02, "books": -0.01, "sports": -0.01},
        "VN": {"fashion": +0.05, "electronics": +0.02, "beauty": +0.02, "home": -0.04, "books": -0.03, "sports": -0.02},
        "JP": {"electronics": +0.05, "home": +0.02, "fashion": -0.02, "beauty": -0.02, "books": -0.02, "sports": -0.01},
        "DE": {"electronics": +0.03, "home": +0.03, "fashion": -0.02, "beauty": -0.02, "books": -0.01, "sports": -0.01},
        "FR": {"fashion": +0.04, "beauty": +0.03, "electronics": -0.02, "home": -0.02, "books": -0.02, "sports": -0.01},
        "GB": {"fashion": +0.03, "electronics": +0.02, "home": 0.0, "beauty": -0.02, "books": -0.02, "sports": -0.01},
        "SG": {"electronics": +0.04, "fashion": +0.02, "beauty": +0.01, "home": -0.03, "books": -0.02, "sports": -0.02},
        "AU": {"sports": +0.03, "electronics": +0.02, "home": +0.01, "fashion": -0.02, "beauty": -0.02, "books": -0.02},
    }
    
    categories_list = ["electronics", "fashion", "home", "beauty", "books", "sports"]
    
    # Generate categories v·ªõi global distribution + country adjustments
    categories = []
    for country in countries:
        # Base distribution
        base_probs = [global_category_distribution[cat] for cat in categories_list]
        
        # Apply country adjustments
        if country in country_category_adjustments:
            adjustments = country_category_adjustments[country]
            adjusted_probs = [
                base_probs[i] + adjustments.get(cat, 0.0)
                for i, cat in enumerate(categories_list)
            ]
            # Normalize ƒë·ªÉ t·ªïng = 1
            total = sum(adjusted_probs)
            adjusted_probs = [p / total for p in adjusted_probs]
        else:
            adjusted_probs = base_probs
        
        # Sample category
        category = np.random.choice(categories_list, p=adjusted_probs)
        categories.append(category)
    
    categories = np.array(categories)
    
    # 4. Amount distribution theo product category - gi√° s·∫£n ph·∫©m e-commerce
    # Electronics: gi√° cao-trung b√¨nh, Home: gi√° trung b√¨nh-cao, Fashion: gi√° trung b√¨nh
    # Beauty: gi√° th·∫•p-trung b√¨nh, Books: gi√° th·∫•p, Sports: gi√° trung b√¨nh-cao
    category_amount_params = {
        "electronics": {
            "mean": 4.8, "sigma": 1.2, "min": 20, "max": 3000,
            "base_price_range": (50, 2000)  # ƒêi·ªán tho·∫°i, laptop, tablet, etc.
        },
        "fashion": {
            "mean": 3.5, "sigma": 1.0, "min": 10, "max": 800,
            "base_price_range": (15, 500)  # Qu·∫ßn √°o, gi√†y d√©p, ph·ª• ki·ªán
        },
        "home": {
            "mean": 4.2, "sigma": 1.1, "min": 25, "max": 2000,
            "base_price_range": (30, 1500)  # ƒê·ªì n·ªôi th·∫•t, trang tr√≠, d·ª•ng c·ª•
        },
        "beauty": {
            "mean": 2.8, "sigma": 0.9, "min": 5, "max": 300,
            "base_price_range": (8, 200)  # M·ªπ ph·∫©m, chƒÉm s√≥c da
        },
        "books": {
            "mean": 2.0, "sigma": 0.7, "min": 3, "max": 100,
            "base_price_range": (5, 50)  # S√°ch, ebook
        },
        "sports": {
            "mean": 4.0, "sigma": 1.0, "min": 20, "max": 1500,
            "base_price_range": (25, 800)  # ƒê·ªì th·ªÉ thao, d·ª•ng c·ª• t·∫≠p
        },
    }
    
    # 4b. Generate amounts tr∆∞·ªõc (s·∫Ω ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh trong temporal patterns n·∫øu c·∫ßn)
    amounts = np.zeros(size, dtype="float32")
    for i, cat in enumerate(categories):
        params = category_amount_params[cat]
        
        # Log-normal distribution v·ªõi bounds
        amount = np.random.lognormal(mean=params["mean"], sigma=params["sigma"])
        amount = np.clip(amount, params["min"], params["max"])
        
        # Electronics v√† Home c√≥ th·ªÉ c√≥ gi√° tr·ªã cao h∆°n (premium products)
        if cat == "electronics":
            if np.random.random() < 0.20:  # 20% l√† premium products
                amount *= np.random.uniform(1.5, 3.0)
                amount = min(amount, params["max"])
        elif cat == "home":
            if np.random.random() < 0.15:  # 15% l√† premium products
                amount *= np.random.uniform(1.4, 2.5)
                amount = min(amount, params["max"])
        
        # Th√™m outliers t·ª± nhi√™n (3-5% t√πy category)
        outlier_prob = 0.05 if cat in ["electronics", "home"] else 0.03
        if np.random.random() < outlier_prob:
            amount *= np.random.uniform(1.5, 3.0)
            amount = min(amount, params["max"] * 2)
        
        amounts[i] = np.round(amount, 2)
    
    # 5. Temporal patterns cho e-commerce v·ªõi ng√†y ƒë√¥i v√† sale events
    now = datetime.utcnow()
    timestamps = []
    
    # Pre-calculate ng√†y ƒë√¥i v√† sale events trong 90 ng√†y g·∫ßn ƒë√¢y
    double_days = []
    sale_days = []
    for days_back in range(90):
        check_date = now - timedelta(days=days_back)
        if is_double_day(check_date):
            double_days.append(days_back)
        if is_sale_event(check_date):
            sale_days.append(days_back)
    
    for i in range(size):
        # Base distribution: nhi·ªÅu transactions g·∫ßn ƒë√¢y h∆°n
        days_ago = np.random.exponential(scale=15)
        days_ago = min(days_ago, 90)
        
        # Ng√†y ƒë√¥i (1/1, 2/2, ..., 12/12) - tƒÉng l∆∞·ª£ng mua h√†ng ƒë√°ng k·ªÉ
        # 25% transactions t·∫≠p trung v√†o c√°c ng√†y ƒë√¥i
        if double_days and np.random.random() < 0.25:
            days_ago = np.random.choice(double_days)
            # TƒÉng amount trong ng√†y ƒë√¥i (mua nhi·ªÅu h∆°n)
            amounts[i] *= np.random.uniform(1.2, 2.0)
            amounts[i] = min(amounts[i], category_amount_params[categories[i]]["max"] * 1.5)
        
        # Sale events - tƒÉng l∆∞·ª£ng mua h√†ng
        # 20% transactions trong c√°c ng√†y sale
        elif sale_days and np.random.random() < 0.20:
            days_ago = np.random.choice(sale_days)
            # TƒÉng amount trong sale (mua nhi·ªÅu h∆°n do gi·∫£m gi√°)
            amounts[i] *= np.random.uniform(1.1, 1.8)
            amounts[i] = min(amounts[i], category_amount_params[categories[i]]["max"] * 1.3)
        
        # Weekend boost cho fashion v√† beauty (30% tƒÉng)
        is_weekend = np.random.random() < 0.3
        if is_weekend and categories[i] in ["fashion", "beauty"]:
            days_ago = min(days_ago, 7)
        
        # End of month boost cho t·∫•t c·∫£ categories (15% tƒÉng)
        is_end_month = np.random.random() < 0.15
        if is_end_month:
            days_ago = np.random.uniform(0, 3)
        
        # Random time trong ng√†y - e-commerce peak v√†o bu·ªïi t·ªëi (19-22h)
        hour = np.random.normal(loc=20, scale=3)  # Peak v√†o bu·ªïi t·ªëi
        hour = np.clip(hour, 0, 23)
        minute = np.random.randint(0, 60)
        second = np.random.randint(0, 60)
        
        timestamp = now - timedelta(
            days=float(days_ago),
            hours=float(23 - hour),
            minutes=float(59 - minute),
            seconds=float(59 - second)
        )
        timestamps.append(timestamp)
    
    # 6. Th√™m m·ªôt s·ªë transactions c√≥ correlation v·ªõi user (gi·ªØ nguy√™n distribution)
    # M·ªôt s·ªë user c√≥ spending pattern nh·∫•t ƒë·ªãnh (8% users) - gi·∫£m ƒë·ªÉ kh√¥ng l√†m m·∫•t distribution
    user_patterns = {}
    unique_user_ids = np.unique(user_ids)
    pattern_users_count = min(int(len(unique_user_ids) * 0.08), 800)
    if pattern_users_count > 0:
        pattern_users = np.random.choice(unique_user_ids, size=pattern_users_count, replace=False)
        
        for user_id in pattern_users:
            # User c√≥ preference category - ∆∞u ti√™n c√°c category ph·ªï bi·∫øn h∆°n
            # ƒê·ªÉ gi·ªØ distribution, 70% ch·ªçn t·ª´ electronics/fashion, 30% t·ª´ c√°c category kh√°c
            if np.random.random() < 0.7:
                preferred_cat = np.random.choice(["electronics", "fashion"], p=[0.52, 0.48])
            else:
                preferred_cat = np.random.choice(["home", "beauty", "books", "sports"], p=[0.4, 0.3, 0.2, 0.1])
            
            # User c√≥ average spending level d·ª±a tr√™n category preference
            cat_params = category_amount_params[preferred_cat]
            avg_spending = np.random.lognormal(mean=cat_params["mean"], sigma=cat_params["sigma"] * 0.8)
            user_patterns[user_id] = {"category": preferred_cat, "avg_amount": avg_spending}
        
        # Apply patterns cho m·ªôt s·ªë transactions c·ªßa pattern users (20% thay v√¨ 25%)
        for i, user_id in enumerate(user_ids):
            if user_id in user_patterns and np.random.random() < 0.20:  # 20% transactions follow pattern
                pattern = user_patterns[user_id]
                new_cat = pattern["category"]
                categories[i] = new_cat
                
                # Recalculate amount v·ªõi category m·ªõi
                params = category_amount_params[new_cat]
                amount = np.random.lognormal(mean=np.log(pattern["avg_amount"]), sigma=0.4)
                amount = np.clip(amount, params["min"], params["max"])
                amounts[i] = np.round(amount, 2)
    
    # 7. T·∫°o DataFrame
    df = pd.DataFrame(
        {
            "id": ids,
            "user_id": user_ids,
            "country": countries,
            "category": categories,
            "amount": amounts,
            "event_time": timestamps,
        }
    )
    
    # Optimize memory usage
    df["country"] = df["country"].astype("category")
    df["category"] = df["category"].astype("category")
    
    logger.info(f"Generated chunk: {len(df)} rows, memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    return df


def upload_chunk_to_s3(df: pd.DataFrame, key: str) -> None:
    """Upload a DataFrame as CSV to S3."""
    s3 = get_s3_client()
    csv_buffer = io.StringIO()
    df.to_csv(csv_buffer, index=False)
    s3.put_object(
        Bucket=settings.s3_bucket_name, Key=key, Body=csv_buffer.getvalue().encode()
    )


def list_raw_objects() -> List[str]:
    s3 = get_s3_client()
    paginator = s3.get_paginator("list_objects_v2")
    keys: List[str] = []
    for page in paginator.paginate(
        Bucket=settings.s3_bucket_name, Prefix=settings.s3_raw_prefix
    ):
        for obj in page.get("Contents", []):
            keys.append(obj["Key"])
    return keys


def read_csv_from_s3(key: str) -> pd.DataFrame:
    s3 = get_s3_client()
    obj = s3.get_object(Bucket=settings.s3_bucket_name, Key=key)
    return pd.read_csv(io.BytesIO(obj["Body"].read()))


# ============================================================================
# DATA PROCESSING METHODS - T·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p x·ª≠ l√Ω d·ªØ li·ªáu
# ============================================================================

def filter_data(df: pd.DataFrame, filters: Optional[Dict] = None) -> pd.DataFrame:
    """
    Ph∆∞∆°ng ph√°p 1: FILTERING - L·ªçc d·ªØ li·ªáu theo ƒëi·ªÅu ki·ªán
    
    Filters:
    - min_amount: L·ªçc giao d·ªãch c√≥ amount >= min_amount
    - max_amount: L·ªçc giao d·ªãch c√≥ amount <= max_amount
    - countries: Danh s√°ch countries c·∫ßn gi·ªØ l·∫°i
    - categories: Danh s√°ch categories c·∫ßn gi·ªØ l·∫°i
    - date_from: L·ªçc t·ª´ ng√†y n√†y
    - date_to: L·ªçc ƒë·∫øn ng√†y n√†y
    """
    logger.info("üîç FILTERING: B·∫Øt ƒë·∫ßu l·ªçc d·ªØ li·ªáu...")
    original_count = len(df)
    
    if filters is None:
        filters = {}
    
    filtered_df = df.copy()
    
    # Filter by amount range
    if "min_amount" in filters:
        filtered_df = filtered_df[filtered_df["amount"] >= filters["min_amount"]]
        logger.info(f"  ‚úì L·ªçc theo min_amount >= {filters['min_amount']}: {len(filtered_df)} rows")
    
    if "max_amount" in filters:
        filtered_df = filtered_df[filtered_df["amount"] <= filters["max_amount"]]
        logger.info(f"  ‚úì L·ªçc theo max_amount <= {filters['max_amount']}: {len(filtered_df)} rows")
    
    # Filter by countries
    if "countries" in filters:
        filtered_df = filtered_df[filtered_df["country"].isin(filters["countries"])]
        logger.info(f"  ‚úì L·ªçc theo countries {filters['countries']}: {len(filtered_df)} rows")
    
    # Filter by categories
    if "categories" in filters:
        filtered_df = filtered_df[filtered_df["category"].isin(filters["categories"])]
        logger.info(f"  ‚úì L·ªçc theo categories {filters['categories']}: {len(filtered_df)} rows")
    
    # Filter by date range
    if "date_from" in filters:
        filtered_df = filtered_df[filtered_df["event_time"] >= filters["date_from"]]
        logger.info(f"  ‚úì L·ªçc t·ª´ ng√†y {filters['date_from']}: {len(filtered_df)} rows")
    
    if "date_to" in filters:
        filtered_df = filtered_df[filtered_df["event_time"] <= filters["date_to"]]
        logger.info(f"  ‚úì L·ªçc ƒë·∫øn ng√†y {filters['date_to']}: {len(filtered_df)} rows")
    
    logger.info(f"üîç FILTERING: Ho√†n th√†nh - {original_count} -> {len(filtered_df)} rows ({len(filtered_df)/original_count*100:.1f}%)")
    return filtered_df


def transform_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    Ph∆∞∆°ng ph√°p 2: TRANSFORMATION - Bi·∫øn ƒë·ªïi d·ªØ li·ªáu
    
    - Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu
    - T√≠nh to√°n c√°c c·ªôt m·ªõi
    - Chu·∫©n h√≥a d·ªØ li·ªáu
    """
    logger.info("üîÑ TRANSFORMATION: B·∫Øt ƒë·∫ßu bi·∫øn ƒë·ªïi d·ªØ li·ªáu...")
    
    transformed_df = df.copy()
    
    # T√≠nh to√°n c·ªôt m·ªõi: amount_category (ph√¢n lo·∫°i theo gi√° tr·ªã)
    def categorize_amount(amt):
        if amt < 10:
            return "small"
        elif amt < 100:
            return "medium"
        elif amt < 1000:
            return "large"
        else:
            return "very_large"
    
    transformed_df["amount_category"] = transformed_df["amount"].apply(categorize_amount)
    logger.info("  ‚úì Th√™m c·ªôt amount_category")
    
    # T√≠nh to√°n c·ªôt m·ªõi: day_of_week
    transformed_df["day_of_week"] = pd.to_datetime(transformed_df["event_time"]).dt.day_name()
    logger.info("  ‚úì Th√™m c·ªôt day_of_week")
    
    # T√≠nh to√°n c·ªôt m·ªõi: month
    transformed_df["month"] = pd.to_datetime(transformed_df["event_time"]).dt.month
    logger.info("  ‚úì Th√™m c·ªôt month")
    
    # T√≠nh to√°n c·ªôt m·ªõi: amount_usd (gi·∫£ s·ª≠ t·ª∑ gi√°)
    exchange_rates = {"US": 1.0, "VN": 0.00004, "JP": 0.0067, "DE": 1.08, "FR": 1.08, "GB": 1.27, "SG": 0.74, "AU": 0.66}
    transformed_df["amount_usd"] = transformed_df.apply(
        lambda row: row["amount"] * exchange_rates.get(row["country"], 1.0), axis=1
    )
    logger.info("  ‚úì Th√™m c·ªôt amount_usd (chuy·ªÉn ƒë·ªïi ti·ªÅn t·ªá)")
    
    logger.info("üîÑ TRANSFORMATION: Ho√†n th√†nh")
    return transformed_df


def validate_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
    """
    Ph∆∞∆°ng ph√°p 3: VALIDATION - Ki·ªÉm tra v√† l√†m s·∫°ch d·ªØ li·ªáu
    
    - Ki·ªÉm tra null values
    - Ki·ªÉm tra duplicate
    - Ki·ªÉm tra gi√° tr·ªã b·∫•t th∆∞·ªùng (outliers)
    - Ki·ªÉm tra ki·ªÉu d·ªØ li·ªáu
    """
    logger.info("‚úÖ VALIDATION: B·∫Øt ƒë·∫ßu ki·ªÉm tra d·ªØ li·ªáu...")
    
    validation_report = {
        "original_rows": len(df),
        "null_counts": {},
        "duplicates": 0,
        "outliers": 0,
        "invalid_rows": 0,
    }
    
    validated_df = df.copy()
    
    # Ki·ªÉm tra null values
    null_counts = validated_df.isnull().sum()
    validation_report["null_counts"] = null_counts[null_counts > 0].to_dict()
    if null_counts.sum() > 0:
        logger.warning(f"  ‚ö† T√¨m th·∫•y {null_counts.sum()} null values")
        validated_df = validated_df.dropna()
        logger.info(f"  ‚úì ƒê√£ x√≥a {len(df) - len(validated_df)} rows c√≥ null")
    
    # Ki·ªÉm tra duplicates
    duplicates = validated_df.duplicated(subset=["id"]).sum()
    validation_report["duplicates"] = int(duplicates)
    if duplicates > 0:
        logger.warning(f"  ‚ö† T√¨m th·∫•y {duplicates} duplicates")
        validated_df = validated_df.drop_duplicates(subset=["id"], keep="first")
        logger.info(f"  ‚úì ƒê√£ x√≥a {duplicates} duplicates")
    
    # Ki·ªÉm tra outliers (s·ª≠ d·ª•ng IQR method)
    Q1 = validated_df["amount"].quantile(0.25)
    Q3 = validated_df["amount"].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 3 * IQR
    upper_bound = Q3 + 3 * IQR
    outliers = ((validated_df["amount"] < lower_bound) | (validated_df["amount"] > upper_bound)).sum()
    validation_report["outliers"] = int(outliers)
    if outliers > 0:
        logger.info(f"  ‚Ñπ T√¨m th·∫•y {outliers} outliers (gi·ªØ l·∫°i ƒë·ªÉ ph√¢n t√≠ch)")
    
    # Ki·ªÉm tra gi√° tr·ªã √¢m (kh√¥ng h·ª£p l·ªá cho amount)
    invalid_amounts = (validated_df["amount"] < 0).sum()
    if invalid_amounts > 0:
        logger.warning(f"  ‚ö† T√¨m th·∫•y {invalid_amounts} rows c√≥ amount < 0")
        validated_df = validated_df[validated_df["amount"] >= 0]
        validation_report["invalid_rows"] = int(invalid_amounts)
        logger.info(f"  ‚úì ƒê√£ x√≥a {invalid_amounts} rows c√≥ amount < 0")
    
    validation_report["final_rows"] = len(validated_df)
    logger.info(f"‚úÖ VALIDATION: Ho√†n th√†nh - {validation_report['original_rows']} -> {validation_report['final_rows']} rows")
    logger.info(f"  üìä B√°o c√°o: {validation_report}")
    
    return validated_df, validation_report


def normalize_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    Ph∆∞∆°ng ph√°p 4: NORMALIZATION - Chu·∫©n h√≥a d·ªØ li·ªáu
    
    - Chu·∫©n h√≥a s·ªë li·ªáu (min-max scaling)
    - Chu·∫©n h√≥a text (lowercase, trim)
    """
    logger.info("üìè NORMALIZATION: B·∫Øt ƒë·∫ßu chu·∫©n h√≥a d·ªØ li·ªáu...")
    
    normalized_df = df.copy()
    
    # Chu·∫©n h√≥a text fields
    if "country" in normalized_df.columns:
        normalized_df["country"] = normalized_df["country"].str.upper().str.strip()
    if "category" in normalized_df.columns:
        normalized_df["category"] = normalized_df["category"].str.lower().str.strip()
    
    logger.info("  ‚úì Chu·∫©n h√≥a text fields (uppercase/lowercase, trim)")
    
    # Min-Max scaling cho amount (0-1 range)
    if "amount" in normalized_df.columns:
        min_amount = normalized_df["amount"].min()
        max_amount = normalized_df["amount"].max()
        if max_amount > min_amount:
            normalized_df["amount_normalized"] = (normalized_df["amount"] - min_amount) / (max_amount - min_amount)
            logger.info("  ‚úì Chu·∫©n h√≥a amount (min-max scaling)")
    
    logger.info("üìè NORMALIZATION: Ho√†n th√†nh")
    return normalized_df


def deduplicate_data(df: pd.DataFrame, subset: List[str] = None) -> pd.DataFrame:
    """
    Ph∆∞∆°ng ph√°p 5: DEDUPLICATION - Lo·∫°i b·ªè d·ªØ li·ªáu tr√πng l·∫∑p
    
    - Lo·∫°i b·ªè duplicate rows
    - C√≥ th·ªÉ ch·ªâ ƒë·ªãnh c√°c c·ªôt ƒë·ªÉ ki·ªÉm tra duplicate
    """
    logger.info("üîÅ DEDUPLICATION: B·∫Øt ƒë·∫ßu lo·∫°i b·ªè duplicates...")
    
    if subset is None:
        subset = ["id"]
    
    original_count = len(df)
    deduplicated_df = df.drop_duplicates(subset=subset, keep="first")
    removed = original_count - len(deduplicated_df)
    
    if removed > 0:
        logger.info(f"  ‚úì ƒê√£ x√≥a {removed} duplicate rows ({removed/original_count*100:.2f}%)")
    else:
        logger.info("  ‚úì Kh√¥ng c√≥ duplicates")
    
    logger.info(f"üîÅ DEDUPLICATION: Ho√†n th√†nh - {original_count} -> {len(deduplicated_df)} rows")
    return deduplicated_df


def aggregate_data(df: pd.DataFrame, group_by: List[str], agg_funcs: Dict = None) -> pd.DataFrame:
    """
    Ph∆∞∆°ng ph√°p 6: AGGREGATION - T·ªïng h·ª£p d·ªØ li·ªáu
    
    - Group by c√°c c·ªôt
    - T√≠nh to√°n c√°c metrics (sum, count, mean, median, etc.)
    """
    logger.info(f"üìä AGGREGATION: B·∫Øt ƒë·∫ßu t·ªïng h·ª£p d·ªØ li·ªáu theo {group_by}...")
    
    if agg_funcs is None:
        agg_funcs = {
            "amount": ["sum", "mean", "count", "min", "max"],
        }
    
    agg_df = df.groupby(group_by).agg(agg_funcs).reset_index()
    
    # Flatten column names
    agg_df.columns = ['_'.join(col).strip('_') if col[1] else col[0] for col in agg_df.columns.values]
    
    logger.info(f"  ‚úì T·ªïng h·ª£p th√†nh {len(agg_df)} groups")
    logger.info(f"üìä AGGREGATION: Ho√†n th√†nh")
    return agg_df


def enrich_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    Ph∆∞∆°ng ph√°p 7: DATA ENRICHMENT - L√†m gi√†u d·ªØ li·ªáu
    
    - Th√™m th√¥ng tin t·ª´ lookup tables
    - Th√™m metadata
    - Th√™m calculated fields
    """
    logger.info("üíé DATA ENRICHMENT: B·∫Øt ƒë·∫ßu l√†m gi√†u d·ªØ li·ªáu...")
    
    enriched_df = df.copy()
    
    # Lookup table cho country names
    country_names = {
        "US": "United States",
        "VN": "Vietnam",
        "JP": "Japan",
        "DE": "Germany",
        "FR": "France",
        "GB": "United Kingdom",
        "SG": "Singapore",
        "AU": "Australia",
    }
    enriched_df["country_name"] = enriched_df["country"].map(country_names)
    logger.info("  ‚úì Th√™m country_name t·ª´ lookup table")
    
    # Th√™m metadata
    enriched_df["processed_at"] = datetime.utcnow()
    logger.info("  ‚úì Th√™m processed_at timestamp")
    
    logger.info("üíé DATA ENRICHMENT: Ho√†n th√†nh")
    return enriched_df


def process_data_comprehensive(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
    """
    H√†m t·ªïng h·ª£p: √Åp d·ª•ng T·∫§T C·∫¢ c√°c ph∆∞∆°ng ph√°p x·ª≠ l√Ω d·ªØ li·ªáu
    
    Th·ª© t·ª± x·ª≠ l√Ω:
    1. Validation (ki·ªÉm tra v√† l√†m s·∫°ch)
    2. Deduplication (lo·∫°i b·ªè duplicates)
    3. Filtering (l·ªçc d·ªØ li·ªáu)
    4. Transformation (bi·∫øn ƒë·ªïi)
    5. Normalization (chu·∫©n h√≥a)
    6. Enrichment (l√†m gi√†u)
    7. Aggregation (t·ªïng h·ª£p)
    """
    logger.info("=" * 80)
    logger.info("üöÄ B·∫ÆT ƒê·∫¶U X·ª¨ L√ù D·ªÆ LI·ªÜU TO√ÄN DI·ªÜN")
    logger.info("=" * 80)
    
    processing_stats = {
        "original_rows": len(df),
        "after_validation": 0,
        "after_deduplication": 0,
        "after_filtering": 0,
        "after_transformation": 0,
        "after_normalization": 0,
        "after_enrichment": 0,
        "final_rows": 0,
    }
    
    # 1. VALIDATION
    df, validation_report = validate_data(df)
    processing_stats["after_validation"] = len(df)
    
    # 2. DEDUPLICATION
    df = deduplicate_data(df, subset=["id"])
    processing_stats["after_deduplication"] = len(df)
    
    # 3. FILTERING (optional - c√≥ th·ªÉ b·ªè qua ho·∫∑c th√™m filters)
    # df = filter_data(df, filters={"min_amount": 0.01})
    processing_stats["after_filtering"] = len(df)
    
    # 4. TRANSFORMATION
    df = transform_data(df)
    processing_stats["after_transformation"] = len(df)
    
    # 5. NORMALIZATION
    df = normalize_data(df)
    processing_stats["after_normalization"] = len(df)
    
    # 6. ENRICHMENT
    df = enrich_data(df)
    processing_stats["after_enrichment"] = len(df)
    
    processing_stats["final_rows"] = len(df)
    
    logger.info("=" * 80)
    logger.info("‚úÖ HO√ÄN TH√ÄNH X·ª¨ L√ù D·ªÆ LI·ªÜU TO√ÄN DI·ªÜN")
    logger.info(f"üìä Th·ªëng k√™: {processing_stats}")
    logger.info("=" * 80)
    
    return df, processing_stats


# ============================================================================
# ETL FUNCTIONS
# ============================================================================

def generate_and_upload_data(total_rows: Optional[int] = None, chunk_size: Optional[int] = None) -> Dict:
    """
    Ch·ªâ sinh v√† upload data l√™n S3 (kh√¥ng ch·∫°y ETL)
    Ph√π h·ª£p cho m√°y 8GB RAM v·ªõi chunking v√† memory optimization
    """
    logger.info("=" * 80)
    logger.info("üì§ B·∫ÆT ƒê·∫¶U SINH V√Ä UPLOAD D·ªÆ LI·ªÜU L√äN S3")
    logger.info("=" * 80)
    
    ensure_bucket()
    
    if total_rows is None:
        total_rows = settings.total_rows
    if chunk_size is None:
        chunk_size = settings.chunk_size
    
    # ƒê·∫£m b·∫£o chunk_size ph√π h·ª£p v·ªõi 8GB RAM
    # ∆Ø·ªõc t√≠nh: m·ªói row ~ 200 bytes, chunk_size 200k rows ~ 40MB
    # V·ªõi 8GB RAM, c√≥ th·ªÉ x·ª≠ l√Ω nhi·ªÅu chunks nh∆∞ng gi·ªØ an to√†n ·ªü 200k
    if chunk_size > 200000:
        chunk_size = 200000
        logger.info(f"‚ö† Chunk size ƒë∆∞·ª£c gi·ªõi h·∫°n ·ªü 200k rows ƒë·ªÉ ph√π h·ª£p v·ªõi 8GB RAM")
    
    total_chunks = math.ceil(total_rows / chunk_size)
    logger.info(f"üìä C·∫•u h√¨nh: {total_rows:,} rows, {chunk_size:,} rows/chunk, {total_chunks} chunks")
    
    current_id = 1
    uploaded_keys: List[str] = []
    total_uploaded = 0
    
    try:
        for chunk_index in range(total_chunks):
            rows = min(chunk_size, total_rows - current_id + 1)
            logger.info(f"üì¶ Chunk {chunk_index + 1}/{total_chunks}: Sinh {rows:,} rows (ID: {current_id} - {current_id + rows - 1})")
            
            # Generate chunk
            df = generate_chunk(current_id, rows)
            
            # Upload to S3
            key = f"{settings.s3_raw_prefix}transactions_{chunk_index:03d}.csv"
            logger.info(f"  ‚¨Ü Uploading to S3: {key}")
            upload_chunk_to_s3(df, key)
            uploaded_keys.append(key)
            total_uploaded += len(df)
            
            logger.info(f"  ‚úÖ Ho√†n th√†nh chunk {chunk_index + 1}: {len(df):,} rows uploaded")
            
            # Gi·∫£i ph√≥ng memory
            del df
            
            current_id += rows
        
        logger.info("=" * 80)
        logger.info(f"‚úÖ HO√ÄN TH√ÄNH: ƒê√£ upload {total_uploaded:,} rows l√™n S3 trong {len(uploaded_keys)} files")
        logger.info("=" * 80)
        
        return {
            "status": "success",
            "total_rows": int(total_uploaded),
            "total_files": len(uploaded_keys),
            "files": uploaded_keys,
        }
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi sinh/upload data: {str(e)}")
        raise


def run_etl() -> dict:
    """
    Full ETL pipeline v·ªõi t·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p x·ª≠ l√Ω d·ªØ li·ªáu:
    generate -> upload -> process (comprehensive) -> aggregate -> load to Postgres
    """
    logger.info("=" * 80)
    logger.info("üöÄ B·∫ÆT ƒê·∫¶U ETL PIPELINE ƒê·∫¶Y ƒê·ª¶")
    logger.info("=" * 80)
    
    ensure_bucket()

    total_rows = settings.total_rows
    chunk_size = settings.chunk_size
    total_chunks = math.ceil(total_rows / chunk_size)

    logger.info(f"üìä C·∫•u h√¨nh ETL: {total_rows:,} rows, {chunk_size:,} rows/chunk")

    # Step 1: Generate & upload raw data (n·∫øu ch∆∞a c√≥)
    logger.info("\nüì§ STEP 1: Generate & Upload Raw Data")
    current_id = 1
    uploaded_keys: List[str] = []
    for chunk_index in range(total_chunks):
        rows = min(chunk_size, total_rows - current_id + 1)
        logger.info(f"  Processing chunk {chunk_index + 1}/{total_chunks}")
        df = generate_chunk(current_id, rows)
        key = f"{settings.s3_raw_prefix}transactions_{chunk_index:03d}.csv"
        upload_chunk_to_s3(df, key)
        uploaded_keys.append(key)
        current_id += rows
        del df  # Gi·∫£i ph√≥ng memory

    # Step 2: Transform v·ªõi t·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p x·ª≠ l√Ω
    logger.info("\nüîÑ STEP 2: Process Data (Comprehensive)")
    agg_frames: List[pd.DataFrame] = []
    
    for idx, key in enumerate(uploaded_keys):
        logger.info(f"  Processing file {idx + 1}/{len(uploaded_keys)}: {key}")
        df = read_csv_from_s3(key)
        
        # √Åp d·ª•ng t·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p x·ª≠ l√Ω
        df_processed, stats = process_data_comprehensive(df)
        
        # Aggregate sau khi x·ª≠ l√Ω
        agg = (
            df_processed.groupby(["country", "category"])["amount"]
            .agg(["sum", "count"])
            .reset_index()
        )
        agg_frames.append(agg)
        
        # Gi·∫£i ph√≥ng memory
        del df, df_processed

    if not agg_frames:
        return {"rows_generated": 0, "rows_aggregated": 0}

    # Step 3: Final aggregation
    logger.info("\nüìä STEP 3: Final Aggregation")
    all_agg = pd.concat(agg_frames, ignore_index=True)
    final_agg = (
        all_agg.groupby(["country", "category"])
        .agg({"sum": "sum", "count": "sum"})
        .reset_index()
    )
    final_agg.rename(
        columns={"sum": "total_amount", "count": "txn_count"}, inplace=True
    )
    logger.info(f"  ‚úì T·ªïng h·ª£p th√†nh {len(final_agg)} groups")

    # Step 4: Load to Postgres
    logger.info("\nüíæ STEP 4: Load to Postgres")
    with engine.begin() as conn:
        conn.execute(text("TRUNCATE TABLE aggregates;"))
        logger.info("  ‚úì ƒê√£ truncate table aggregates")
        
        # Batch insert ƒë·ªÉ t·ªëi ∆∞u
        values = [
            {
                "country": row["country"],
                "category": row["category"],
                "total_amount": float(row["total_amount"]),
                "txn_count": int(row["txn_count"]),
            }
            for _, row in final_agg.iterrows()
        ]
        
        # Insert in batches
        batch_size = 100
        for i in range(0, len(values), batch_size):
            batch = values[i:i + batch_size]
            for val in batch:
                conn.execute(
                    text(
                        """
                        INSERT INTO aggregates (country, category, total_amount, txn_count)
                        VALUES (:country, :category, :total_amount, :txn_count)
                        """
                    ),
                    val,
                )
        
        logger.info(f"  ‚úì ƒê√£ insert {len(values)} records v√†o Postgres")

    logger.info("=" * 80)
    logger.info("‚úÖ HO√ÄN TH√ÄNH ETL PIPELINE")
    logger.info("=" * 80)

    return {
        "rows_generated": int(total_rows),
        "rows_aggregated": int(final_agg["txn_count"].sum()),
        "groups": int(len(final_agg)),
    }


